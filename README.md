Autism Behavior Analysis:

Our project plan is to develop a human-centric AI system capable of analyzing diverse multimodal behavioral data of children with autism spectrum disorder (ASD) by utilizing hybrid cutting-edge Artificial Intelligence (AI) technologies. This particular initiative focuses on bridging the gap in understanding between caregivers (educators or parents) and their autistic children by augmenting autism care through AI-powered personalized behavioral analysis and prediction. Our work involves leveraging state of-the-art AI technologies to understand, monitor, and predict behavioral patterns and outcomes in children 2-5 years old on the autism spectrum in order to create personalized intervention strategies based on behavioral insights. For study and analysis, we will use data of multiple modalities such as the children’s day-to-day audio, video, environmental, and electrodermal activity data, as well as their clinical specialists’ and Applied Behavior Analysis (ABA) therapist’s reports. For evaluation, we will use evidence-based techniques in psychology to test improvement in their behavior, language, and communication functions through longitudinal study. Our goal to multimodal AI system development encompasses a wide range of tools and technologies in the field, including but not limited to, machine learning (ML) - deep learning (DL) algorithms, natural language processing (NLP) techniques (such as large language models - LLMs), computer vision, and vision and sensor-based monitoring.

Motivated from the growing requirements to understand the emotional and behavioral cues of children with ASD, our work focuses on bridging the communication gap between children and their caregivers. ASD is a developmental and neurological condition characterized by challenges in social skills, communication, and repetitive behaviors. Intellectual disabilities and other social communication difficulties also vary but often include decreased facial expression (such as social smiling), eye contact, joint attention, gestures, and differences in sensory responses. These difficulties make it harder for children with ASD to express their needs, often leading to distress and frustration in both the caretaker and (especially) in the child. Such kind of understanding of emotional states is an important step to addressing their unique needs. About 1 in 36 U.S. children were identified with ASD in 2020, increased from 1 in 44 in 2018 as Centers for Disease Control and Prevention (CDC) estimated. As per World Health Organization (WHO), 1 out of 100 children worldwide have autism. Currently, social awareness and research are improving the situation while some of the important areas are still need enhancement. One of most important area is to understanding the emotion of the ASD children with multimodality.

We have run preliminary data analysis using ResNet34, 50, and DeepFace models on two publicly available facial expression datasets [1,2] used in prior ASD detection research. Our preliminary data analysis shows that facial expression analysis alone gives ambiguous inferences about the positive or negative emotional valence. In order to increase the accuracy and reliability of prediction from behavior analysis, as is our target here, we need to consider multiple modes of data concurrently. Hence, this proposed project focuses on data collection in the form of audio, video, environmental, and physiological data.

To collect data, we are collaborating with multiple ABA (Applied Behavior Analysis) centers and other ASD-related healthcare institutions in the New Orleans and greater Louisiana area to set up and gather a diverse speech, behavior, background information, and medical records dataset for children with ASD. To Collect multimodal behavioral data, in the form of eye-tracking, language samples, and video captured movements, along with environmental factors (lighting condition, background sound, time of the day, etc.) by installing vision and sensor systems at ABA centers/clinics within the metro area. we plan to use equipment like multiple Basler cameras, Depth camera, omnidirectional microphone and Cardioid mic to pick up sound regardless of its direction, air monitor, smart watch or wearable motion sensors for electrodermal activity (EDA) measurement for data collection.

References:
[1] Alamgir FM, Saif SM, Hossain SM, Al Hadi A, Alam MS. Facial Expression Database of Autism Spectrum Disorder Children.
[2] MD. Shafiul Alam, "Facial Expression Recognition Dataset, "Kaggle, https://www.kaggle.com/datasets/shafi420/facial. Accessed: Nov. 5, 2024.
